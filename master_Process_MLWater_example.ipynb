{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-DC ML water masks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "from utils.genprepWater import per_scene_wofs\n",
    "from utils.prep_utils import s3_list_objects_paths\n",
    "\n",
    "from utils.genprepMLWater import genprepmlwater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16c353e3c1673eb9556e\n",
      "s3-uk-1.sa-catapult.co.uk\n"
     ]
    }
   ],
   "source": [
    "# # should encode to env vars as with docker, not csv\n",
    "# os.environ['AWS_ACCESS_KEY_ID']= str(pd.read_csv('../aws_creds.csv').AWSAccessKeyId.values[0])\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']= str(pd.read_csv('../aws_creds.csv').AWSSecretKey.values[0])\n",
    "# # os.environ['AWS_S3_ENDPOINT']=str(pd.read_csv('../aws_creds.csv').AWS_S3_ENDPOINT.values[0])\n",
    "print(os.environ['AWS_ACCESS_KEY_ID'])\n",
    "print(os.environ['AWS_S3_ENDPOINT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example job**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "essential **input vars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential input vars\n",
    "s3_bucket = 'public-eo-data'\n",
    "s3_dir = 'common_sensing/tom/mlperscene_test3/'\n",
    "inter_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find **list of scene yamls** - this is essentially input to the job list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11680"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_dir_allprods = 'common_sensing/fiji/'\n",
    "# prep to single yaml path\n",
    "s3_paths = s3_list_objects_paths(s3_bucket, s3_dir_allprods)\n",
    "yamls = [i for i in s3_paths if i.split('/')[-1][-4:] == 'yaml']\n",
    "len(yamls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter to **yamls of test scenes** - i.e. scenes from desired sensors that are largely spatially covered by and within preceeding year to Annual Wofs Water Summary test aoi and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR  |  L7  |  L8  |  S2  |  S1\n",
      "2000 6 0 0 0\n",
      "2001 11 0 0 9\n",
      "2002 7 0 0 3\n",
      "2003 7 0 0 0\n",
      "2004 13 0 0 0\n",
      "2005 12 0 0 0\n",
      "2006 14 0 0 0\n",
      "2007 8 0 0 0\n",
      "2008 11 0 0 0\n",
      "2009 10 0 0 0\n",
      "2010 16 0 0 0\n",
      "2011 7 0 0 0\n",
      "2012 15 0 0 0\n",
      "2013 14 15 0 0\n",
      "2014 0 22 0 0\n",
      "2015 0 23 0 0\n",
      "2016 0 22 0 0\n",
      "2017 0 23 0 69\n",
      "2018 0 23 34 86\n",
      "2019 0 16 82 86\n",
      "2020 0 0 0 12\n"
     ]
    }
   ],
   "source": [
    "ys = ['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020']\n",
    "print(\"YEAR  |  L7  |  L8  |  S2  |  S1\")\n",
    "for y in ys:\n",
    "    l7_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'landsat_7') & ('074072' in i) & (y == i.split('/')[3].split('_')[-1][:4])])\n",
    "    l8_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'landsat_8') & ('074072' in i) & (y == i.split('/')[3].split('_')[-1][:4])])\n",
    "    s2_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'sentinel_2') & ('60KXF' in i ) & (y in i)])\n",
    "    s1_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'sentinel_1') & (y in i)])\n",
    "    \n",
    "    print(y, len(l7_ymls), len(l8_ymls), len(s2_ymls), len(s1_ymls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 82)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'landsat_8') & ('074072' in i) & ('2019' == i.split('/')[3].split('_')[-1][:4])])\n",
    "s2_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'sentinel_2') & ('60KXF' in i ) & ('2019' in i)])\n",
    "l7_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'landsat_7') & ('074072' in i) & ('2013' == i.split('/')[3].split('_')[-1][:4])])\n",
    "s1_ymls = sorted([i for i in yamls if (i.split('/')[2] == 'sentinel_1') & ('2019' in i)])\n",
    "len(s1_ymls), len(l8_ymls), len(s2_ymls), len(l7_ymls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter yamls to find list of **available annual water summary** products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wofssummary_ymls = [i for i in yamls if (i.split('/')[2] == 'wofs_summary') & ('2013' in i)]\n",
    "len(wofssummary_ymls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab example from each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_sensing/fiji/sentinel_1/S1A_IW_GRDH_1SDV_20191229T174052/datacube-metadata.yaml\n",
      "common_sensing/fiji/sentinel_2/S2B_MSIL2A_20191115T221939_T60KXF/datacube-metadata.yaml\n",
      "common_sensing/fiji/landsat_8/LC08_L1TP_074072_20190910/datacube-metadata.yaml\n",
      "common_sensing/fiji/landsat_7/LE07_L1TP_074072_20131104/datacube-metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "l = [s1_ymls[-1],s2_ymls[-2],l8_ymls[-1],l7_ymls[-1]]\n",
    "for i in l: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example **job function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['common_sensing/fiji/sentinel_1/S1A_IW_GRDH_1SDV_20191229T174052/datacube-metadata.yaml', 'common_sensing/fiji/sentinel_2/S2B_MSIL2A_20191115T221939_T60KXF/datacube-metadata.yaml', 'common_sensing/fiji/landsat_8/LC08_L1TP_074072_20190910/datacube-metadata.yaml', 'common_sensing/fiji/landsat_7/LE07_L1TP_074072_20131104/datacube-metadata.yaml']\n",
      "SENTINEL_2 ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'scene_classification']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 2building tree 2 of 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    9.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing scene S2B_MSIL2A_20191115T221939_T60KXF_mlwater\n",
      "Scene path ../data/S2B_MSIL2A_20191115T221939_T60KXF_tmp/S2B_MSIL2A_20191115T221939_T60KXF_mlwater/\n",
      "{'watermask': {'path': 'S2B_MSIL2A_20191115T221939_T60KXF_watermask.tif'}, 'waterprob': {'path': 'S2B_MSIL2A_20191115T221939_T60KXF_waterprob.tif'}}\n",
      "{'spatial_reference': 'PROJCS[\"WGS 84 / UTM zone 60S\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",177],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",10000000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32760\"]]', 'geo_ref_points': {'ul': {'x': 600005.0, 'y': 8100035.0}, 'ur': {'x': 709795.0, 'y': 8100035.0}, 'll': {'x': 600005.0, 'y': 7990245.0}, 'lr': {'x': 709795.0, 'y': 7990245.0}}} {'ul': {'lon': -17.182303526159554, 'lat': 177.94039370455036}, 'ur': {'lon': -17.174853308574782, 'lat': 178.97247120667822}, 'll': {'lon': -18.174553081137606, 'lat': 177.94556763367254}, 'lr': {'lon': -18.166644279587533, 'lat': 178.98331681851872}}\n",
      "not boo\n",
      "['common_sensing/fiji/sentinel_1/S1A_IW_GRDH_1SDV_20191229T174052/datacube-metadata.yaml', 'common_sensing/fiji/sentinel_2/S2B_MSIL2A_20191115T221939_T60KXF/datacube-metadata.yaml', 'common_sensing/fiji/landsat_8/LC08_L1TP_074072_20190910/datacube-metadata.yaml', 'common_sensing/fiji/landsat_7/LE07_L1TP_074072_20131104/datacube-metadata.yaml']\n",
      "LANDSAT_8 ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'pixel_qa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 2building tree 2 of 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing scene LC08_L1TP_074072_20190910_mlwater\n",
      "Scene path ../data/LC08_L1TP_074072_20190910_tmp/LC08_L1TP_074072_20190910_mlwater/\n",
      "{'watermask': {'path': 'LC08_L1TP_074072_20190910_watermask.tif'}, 'waterprob': {'path': 'LC08_L1TP_074072_20190910_waterprob.tif'}}\n",
      "{'spatial_reference': 'PROJCS[\"WGS 84 / UTM zone 60N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",177],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32660\"]]', 'geo_ref_points': {'ul': {'x': 584100.0, 'y': -1802100.0}, 'ur': {'x': 815100.0, 'y': -1802100.0}, 'll': {'x': 584100.0, 'y': -2036100.0}, 'lr': {'x': 815100.0, 'y': -2036100.0}}} {'ul': {'lon': -16.29835187851683, 'lat': 177.787200998347}, 'ur': {'lon': -16.27926129700365, 'lat': 179.94811858858546}, 'll': {'lon': -18.413326825528546, 'lat': 177.79628096291827}, 'lr': {'lon': -18.39159673111109, 'lat': 179.98206696251242}}\n",
      "not boo\n",
      "['common_sensing/fiji/sentinel_1/S1A_IW_GRDH_1SDV_20191229T174052/datacube-metadata.yaml', 'common_sensing/fiji/sentinel_2/S2B_MSIL2A_20191115T221939_T60KXF/datacube-metadata.yaml', 'common_sensing/fiji/landsat_8/LC08_L1TP_074072_20190910/datacube-metadata.yaml', 'common_sensing/fiji/landsat_7/LE07_L1TP_074072_20131104/datacube-metadata.yaml']\n",
      "LANDSAT_7 ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'pixel_qa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 2\n",
      "building tree 2 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   23.9s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing scene LE07_L1TP_074072_20131104_mlwater\n",
      "Scene path ../data/LE07_L1TP_074072_20131104_tmp/LE07_L1TP_074072_20131104_mlwater/\n",
      "{'watermask': {'path': 'LE07_L1TP_074072_20131104_watermask.tif'}, 'waterprob': {'path': 'LE07_L1TP_074072_20131104_waterprob.tif'}}\n",
      "{'spatial_reference': 'PROJCS[\"WGS 84 / UTM zone 60N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",177],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32660\"]]', 'geo_ref_points': {'ul': {'x': 575100.0, 'y': -1812600.0}, 'ur': {'x': 818700.0, 'y': -1812600.0}, 'll': {'x': 575100.0, 'y': -2023200.0}, 'lr': {'x': 818700.0, 'y': -2023200.0}}} {'ul': {'lon': -16.393562856473512, 'lat': 177.7033027167065}, 'ur': {'lon': -16.373581536402064, 'lat': 179.98320564662208}, 'll': {'lon': -18.29707968075992, 'lat': 177.71059468509867}, 'lr': {'lon': -18.274627481238337, 'lat': -179.98591753206225}}\n",
      "not boo\n"
     ]
    }
   ],
   "source": [
    "for sc in l[1:]:\n",
    "    print(l)\n",
    "    genprepmlwater(sc, wofssummary_ymls[0], s3_bucket=s3_bucket, s3_dir=s3_dir, inter_dir=inter_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Original Walkthrough** \n",
    "of process & tranlation into function - appreciate lots more could be done to be O/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "# from rasterio.transform import Affine\n",
    "from affine import Affine\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import logging\n",
    "import logging.handlers\n",
    "from dateutil.parser import parse\n",
    "import uuid\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import gdal\n",
    "import gc\n",
    "import traceback\n",
    "\n",
    "# ml stuff\n",
    "from sklearn_xarray import wrap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from utils.dc_water_classifier import wofs_classify\n",
    "from utils.dc_clean_mask import landsat_qa_clean_mask\n",
    "from utils.prep_utils import s3_list_objects, s3_download, s3_upload_cogs, create_yaml, cog_translate, get_geometry\n",
    "from utils.dc_import_export import export_xarray_to_geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resamp_bands(xr, xrs):\n",
    "    if xr.attrs['res'] == xrs[0].attrs['res']:\n",
    "        return xr\n",
    "    else:\n",
    "        return xr.interp(x=xrs[0]['x'], y=xrs[0]['y'], method='nearest') # nearest as exclusively upsampling\n",
    "    \n",
    "def rename_bands(in_xr, des_bands, position):\n",
    "    in_xr.name = des_bands[position]\n",
    "    return in_xr\n",
    "\n",
    "def get_valid(ds, prod):\n",
    "    # Identify pixels with valid data\n",
    "    if 'LANDSAT_8' in prod:\n",
    "        good_quality = (\n",
    "            (ds.pixel_qa == 322)  | # clear\n",
    "            (ds.pixel_qa == 386)  |\n",
    "            (ds.pixel_qa == 834)  |\n",
    "            (ds.pixel_qa == 898)  |\n",
    "            (ds.pixel_qa == 1346) |\n",
    "            (ds.pixel_qa == 324)  | # water\n",
    "            (ds.pixel_qa == 388)  |\n",
    "            (ds.pixel_qa == 836)  |\n",
    "            (ds.pixel_qa == 900)  |\n",
    "            (ds.pixel_qa == 1348)\n",
    "        )\n",
    "    elif prod in [\"LANDSAT_7\", \"LANDSAT_5\", \"LANDSAT_4\"]:    \n",
    "        good_quality = (\n",
    "            (ds.pixel_qa == 66)   | # clear\n",
    "            (ds.pixel_qa == 130)  |\n",
    "            (ds.pixel_qa == 68)   | # water\n",
    "            (ds.pixel_qa == 132)  \n",
    "        )\n",
    "    elif 'SENTINEL_2' in prod:\n",
    "        good_quality = (\n",
    "            (ds.scene_classification == 2) | # mask in DARK_AREA_PIXELS\n",
    "#             (ds.scene_classification == 3) | # mask in CLOUD_SHADOWS\n",
    "            (ds.scene_classification == 4) | # mask in VEGETATION\n",
    "            (ds.scene_classification == 5) | # mask in NOT_VEGETATED\n",
    "            (ds.scene_classification == 6) | # mask in WATER\n",
    "            (ds.scene_classification == 7)   # mask in UNCLASSIFIED\n",
    "        )\n",
    "    elif 'WOFS_SUMMARY' in prod:\n",
    "        good_quality = (\n",
    "            (ds >= 0)\n",
    "        )\n",
    "    elif 'SENTINEL_1' in prod:\n",
    "        good_quality = (\n",
    "            (ds.vv != 0)\n",
    "        )\n",
    "    return good_quality\n",
    "\n",
    "def get_ref_channel(prod):\n",
    "    if ('LANDSAT' in prod) | ('SENTINEL_2' in prod): return 'swir1'\n",
    "    elif 'SENTINEL_1' in prod: return 'vv'\n",
    "    \n",
    "def get_qa_channel(prod):\n",
    "    if 'LANDSAT' in prod: return 'pixel_qa'\n",
    "    elif 'SENTINEL_2' in prod: return 'scene_classification'\n",
    "    elif 'SENTINEL_1' in prod: return 'layovershadow_mask'\n",
    "    \n",
    "\n",
    "def band_name_water(prod_path):\n",
    "    \"\"\"\n",
    "    Determine l8 band of individual product from product name\n",
    "    from path to specific product file\n",
    "    \"\"\"\n",
    "\n",
    "    prod_name = os.path.basename(prod_path)\n",
    "    parts = prod_name.split('_')\n",
    "    prod_name = f\"{parts[-2]}_{parts[-1][:-4]}\"\n",
    "\n",
    "    prod_map = {\n",
    "        \"watermask\": 'watermask',\n",
    "        \"waterprob\": 'waterprob'\n",
    "    }\n",
    "    layer_name = prod_map[prod_name]\n",
    "    return layer_name\n",
    "\n",
    "def yaml_prep_water(scene_dir, original_yml):\n",
    "    \"\"\"\n",
    "    Prepare individual wofs directory containing L8/S2/S1 cog water products.\n",
    "    \"\"\"\n",
    "    # scene_name = scene_dir.split('/')[-2][:26]\n",
    "    scene_name = scene_dir.split('/')[-2]\n",
    "    print ( \"Preparing scene {}\".format(scene_name) )\n",
    "    print ( \"Scene path {}\".format(scene_dir) )\n",
    "    \n",
    "    # find all cog prods\n",
    "    prod_paths = glob.glob(scene_dir + '*water*.tif')\n",
    "    # print ( 'paths: {}'.format(prod_paths) )\n",
    "    # for i in prod_paths: print ( i )\n",
    "    \n",
    "    # date time assumed eqv for start and stop - this isn't true and could be \n",
    "    # pulled from .xml file (or scene dir) not done yet for sake of progression\n",
    "    t0=parse(str(datetime.strptime(original_yml['extent']['center_dt'], '%Y-%m-%d %H:%M:%S')))\n",
    "    # print ( t0 )\n",
    "    t1=t0\n",
    "    # print ( t1 )\n",
    "    \n",
    "    # name image product\n",
    "    images = {\n",
    "        prod_path.split('_')[-1][:9]: {\n",
    "            'path': str(prod_path.split('/')[-1])\n",
    "        } for prod_path in prod_paths\n",
    "    }\n",
    "    print ( images )\n",
    "\n",
    "    # trusting bands coaligned, use one to generate spatial bounds for all\n",
    "    projection, extent = get_geometry(os.path.join(str(scene_dir), images['watermask']['path']))\n",
    "#     extent = \n",
    "    print(projection, extent)\n",
    "    \n",
    "    new_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"{scene_name}_water\"))\n",
    "    \n",
    "    return {\n",
    "        'id': new_id,\n",
    "        'processing_level': original_yml['processing_level'],\n",
    "        'product_type': \"mlwater\",\n",
    "        'creation_dt': str(datetime.today().strftime('%Y-%m-%d %H:%M:%S')),\n",
    "        'platform': {  \n",
    "            'code': original_yml['platform']['code']\n",
    "        },\n",
    "        'instrument': {\n",
    "            'name': original_yml['instrument']['name']\n",
    "        },\n",
    "        'extent': {\n",
    "            'coord': original_yml['extent']['coord'],\n",
    "            'from_dt': str(t0),\n",
    "            'to_dt': str(t1),\n",
    "            'center_dt': str(t0 + (t1 - t0) / 2)\n",
    "        },\n",
    "        'format': {\n",
    "            'name': 'GeoTiff'\n",
    "        },\n",
    "        'grid_spatial': {\n",
    "            'projection': projection\n",
    "        },\n",
    "        'image': {\n",
    "            'bands': images\n",
    "        },\n",
    "        'lineage': {\n",
    "            'source_datasets': original_yml['lineage']['source_datasets'],\n",
    "        }  \n",
    "    }\n",
    "\n",
    "\n",
    "def mlwater(optical_yaml_path, summary_yaml_path, inter_dir='../data/', s3_bucket='public-eo-data', s3_dir='common_sensing/fiji/mlwater_test/',\n",
    "            mask=None, output_crs=None):\n",
    "    \"\"\"\n",
    "    optical_yaml_path: dc yml metadata of single image within S3 bucket\n",
    "    summary_yaml_path: dc yml metadata of wofs-like summary product within S3 bucket\n",
    "    \"\"\"\n",
    "\n",
    "    try: \n",
    "        \n",
    "        scene_name = os.path.dirname(optical_yaml_path).split('/')[-1]\n",
    "        \n",
    "        inter_dir = f\"{inter_dir}{scene_name}_tmp/\"\n",
    "        os.makedirs(inter_dir, exist_ok=True)\n",
    "        cog_dir = f\"{inter_dir}{scene_name}/\"\n",
    "        os.makedirs(cog_dir, exist_ok=True)\n",
    "        \n",
    "        yml = f'{inter_dir}datacube-metadata.yaml'\n",
    "        yml_summary = f'{inter_dir}datacube-metadata_watersummary.yaml'\n",
    "        \n",
    "        des_band_refs = {\n",
    "            \"LANDSAT_8\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "            \"LANDSAT_7\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "            \"LANDSAT_5\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "            \"LANDSAT_4\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "            \"SENTINEL_2\": ['blue','green','red','nir','swir1','swir2','scene_classification'],\n",
    "            \"SENTINEL_1\": ['vv','vh','layovershadow_mask'],\n",
    "            \"WOFS_SUMMARY\": ['pc']}\n",
    "        \n",
    "        if not os.path.exists(yml):\n",
    "            s3_download(s3_bucket, optical_yaml_path, yml)\n",
    "            with open (yml) as stream: yml_meta = yaml.safe_load(stream)\n",
    "            satellite = yml_meta['platform']['code'] # helper to generalise masking \n",
    "            des_bands = des_band_refs[satellite]\n",
    "            print(satellite, des_bands)\n",
    "            band_paths_s3 = [os.path.dirname(optical_yaml_path)+'/'+yml_meta['image']['bands'][b]['path'] for b in des_bands ]\n",
    "            band_paths_local = [inter_dir+os.path.basename(i) for i in band_paths_s3]\n",
    "            for s3, loc in zip(band_paths_s3, band_paths_local): \n",
    "                if not os.path.exists(loc):\n",
    "                    s3_download(s3_bucket, s3, loc)\n",
    "        elif os.path.exists(yml):\n",
    "            with open (yml) as stream: yml_meta = yaml.safe_load(stream)\n",
    "            satellite = yml_meta['platform']['code'] # helper to generalise masking \n",
    "            des_bands = des_band_refs[satellite]\n",
    "            print(satellite, des_bands)\n",
    "        else:\n",
    "            print('boo')\n",
    "\n",
    "        # FIND & DOWNLOAD WATER SUMMARY DATA\n",
    "        if not os.path.exists(yml_summary):\n",
    "            s3_download(s3_bucket, summary_yaml_path, yml_summary)\n",
    "            with open (yml_summary) as stream: yml_summary_meta = yaml.safe_load(stream)\n",
    "            summary = yml_summary_meta['platform']['code']\n",
    "            des_bands_summary = des_band_refs[summary]\n",
    "            print(summary, des_bands_summary)\n",
    "            band_paths_s3 = [os.path.dirname(summary_yaml_path)+'/'+yml_summary_meta['image']['bands'][b]['path'] for b in des_bands_summary ]\n",
    "            band_paths_local = [inter_dir+os.path.basename(i) for i in band_paths_s3]\n",
    "            for s3, loc in zip(band_paths_s3, band_paths_local): \n",
    "                if not os.path.exists(loc):\n",
    "                    s3_download(s3_bucket, s3, loc)\n",
    "        elif os.path.exists(yml_summary):\n",
    "            s3_download(s3_bucket, summary_yaml_path, yml_summary)\n",
    "            with open (yml_summary) as stream: yml_summary_meta = yaml.safe_load(stream)\n",
    "            summary = yml_summary_meta['platform']['code']\n",
    "            des_bands_summary = des_band_refs[summary]\n",
    "            print(summary, des_bands_summary)\n",
    "            band_paths_s3 = [os.path.dirname(summary_yaml_path)+'/'+yml_summary_meta['image']['bands'][b]['path'] for b in des_bands_summary ]\n",
    "            band_paths_local = [inter_dir+os.path.basename(i) for i in band_paths_s3]\n",
    "            for s3, loc in zip(band_paths_s3, band_paths_local): \n",
    "                if not os.path.exists(loc):\n",
    "                    s3_download(s3_bucket, s3, loc)\n",
    "        else:\n",
    "            print('boo')\n",
    "\n",
    "        ref_channel = get_ref_channel(satellite)\n",
    "        qa_channel = get_qa_channel(satellite)\n",
    "        print(ref_channel, qa_channel)\n",
    "\n",
    "        # LOAD & PREP IMAGE DATA\n",
    "        o_bands_data = [ xr.open_rasterio(inter_dir + yml_meta['image']['bands'][b]['path']) for b in des_bands ] # loading\n",
    "        o_bands_data = [ resamp_bands(i, o_bands_data) for i in o_bands_data ] # resamp 2 match band 1\n",
    "        bands_data = xr.merge([rename_bands(bd, des_bands, i) for i,bd in enumerate(o_bands_data)]).rename({'band': 'time'}) # ensure band names & dims consistent\n",
    "        bands_data = bands_data.assign_attrs(o_bands_data[0].attrs) # crs etc. needed later\n",
    "        bands_data['time'] = [datetime.strptime(yml_meta['extent']['center_dt'], '%Y-%m-%d %H:%M:%S')] # time dim needed for wofs\n",
    "\n",
    "        img_data = bands_data.isel(time = 0).drop(['time'])\n",
    "        bands_data = None\n",
    "        o_bands_data = None\n",
    "        \n",
    "        # LOAD & PREP WATER SUMMARY DATA\n",
    "        o_bands_data = [ xr.open_rasterio(inter_dir + yml_summary_meta['image']['bands'][b]['path']) for b in des_bands_summary ] # loading\n",
    "        o_bands_data = [ resamp_bands(i, o_bands_data) for i in o_bands_data ]\n",
    "        sum_data = xr.merge([rename_bands(bd, des_bands_summary, i) for i,bd in enumerate(o_bands_data)]).rename({'band': 'time'}) # ensure band names & dims consistent\n",
    "        sum_data = sum_data.assign_attrs(o_bands_data[0].attrs) # crs etc. needed later\n",
    "        sum_data['time'] = [datetime.strptime(yml_summary_meta['extent']['center_dt'], '%Y-%m-%d %H:%M:%S')] # time dim needed for wofs\n",
    "\n",
    "        class_data = sum_data.isel(time = 0).drop(['time']).pc\n",
    "        sum_data = None\n",
    "        o_bands_data = None\n",
    "        \n",
    "        # REPROJECT & ALIGN CRS+DIMS\n",
    "        class_data = class_data.rio.reproject_match(img_data[ref_channel]) # repro + align grid\n",
    "        img_data, class_data = xr.align(img_data, class_data, join=\"override\") # force alignment of x,y precision\n",
    "        \n",
    "        if satellite == 'SENTINEL_1': # catch s1 and scall and re-fromat dtype\n",
    "            att = img_data.attrs\n",
    "            img_data = img_data*100\n",
    "            img_data = img_data.astype('int16')\n",
    "            img_data.attrs = att\n",
    "        \n",
    "        # VALID REGION MASKS\n",
    "        clearskymask_img = get_valid(img_data, satellite) # img nd mask\n",
    "        clearskymask_class = get_valid(class_data, summary) # water nd mask\n",
    "        clearskymask_train = clearskymask_class.where(clearskymask_class == False, False) # empty mask\n",
    "        clearskymask_train = clearskymask_train.where((clearskymask_img == False) | (clearskymask_class == False), True) # inner true mask\n",
    "        \n",
    "        # ASSIGN WATER/NON WATER CLASS LABELS\n",
    "        water_thresh = 50 # 50% persistence in annual summary\n",
    "        class_data = class_data.where((class_data < water_thresh) | (clearskymask_class == False), 100) # fix > prob to water\n",
    "        class_data = class_data.where((class_data >= water_thresh) | (clearskymask_class == False), 0) # fix < prob to no water \n",
    "        \n",
    "        # MASK TO TRAINING SAMPLES W/ IMPUTED ND\n",
    "        train_data = img_data # dup as use img 4 implementation later\n",
    "        train_data['waterclass'] = class_data # add channel for water mask\n",
    "        train_data = train_data.where(clearskymask_train == True, -9999).drop([qa_channel]) # apply inner mask\n",
    "        \n",
    "        unique, counts = np.unique(train_data.waterclass, return_counts=True)\n",
    "        if (counts[0] < 500) | (counts[1] < 5000):\n",
    "            raise Exception(f'no class labels should be >5000 for ok classifier. no. training class samples: {counts[0]}{counts[1]}')\n",
    "        \n",
    "        # SPEC & TRAIN MODEL\n",
    "        Y = train_data.waterclass.stack(z=['x','y']) # stack into 1-d arr\n",
    "        X = train_data.drop(['waterclass']).stack(z=['x','y']).to_array().transpose() # stack into transposed 2-d arr\n",
    "        # very shallow classifier - this is a super easy problem & we want it to be fast\n",
    "        wrapper = wrap(RandomForestClassifier(n_estimators=2, \n",
    "                                       bootstrap = True,\n",
    "                                       max_features = 'sqrt',\n",
    "                                       max_depth=5,\n",
    "                                       n_jobs=2,\n",
    "                                       verbose=2\n",
    "                                      ))\n",
    "        wrapper.estimator.fit(X, Y) # do training\n",
    "        \n",
    "        # MASK TO FULL VALID IMAGE FOR IMPLEMENTATION\n",
    "        img_data = img_data.drop([qa_channel,'waterclass']) # not sure how these ended up in here(?)\n",
    "        img_data = img_data.where(clearskymask_img == True, -9999) # apply just the img mask this time\n",
    "        \n",
    "        # PREDICT + ASSIGN CONFIDENCE\n",
    "        X = img_data.stack(z=['x','y']).to_array().transpose() # stack into transposed 2-d arr\n",
    "\n",
    "        pred = wrapper.estimator.predict(X) # gen class predictions\n",
    "        pred[pred==100] = 1 # refactor water from 100 to 1\n",
    "        prob = wrapper.estimator.predict_proba(X)[:,2]*100 # gen confidence in assigned labels as int\n",
    "        \n",
    "        # RESHAPE OUTPUTS INTO IMAGE\n",
    "        vars_0 = [i for i in X.transpose().to_dataset(dim='variable').data_vars] # get list of vars within img\n",
    "        X_t = X.transpose().to_dataset(dim='variable') # recreate xrds (but no unstacking yet as need to drop in model outputs)\n",
    "        X_t[vars_0[0]].data = pred # add class predictions as first channel\n",
    "        X_t[vars_0[1]].data = prob # add confidence as second channel\n",
    "        X_t = X_t.rename({vars_0[0]:'water_mask',vars_0[1]:'water_prob'}).drop(vars_0[2:]).unstack('z').transpose().astype('int16') # rename + drop vars + unstack xy dims back to 3-d xrds + transpose predictions back into correct orientation\n",
    "        X_t = X_t.where(clearskymask_img,-9999) # ensure probs rm 4 nd regions\n",
    "        X_t.attrs = img_data.attrs\n",
    "\n",
    "        # EXPORT\n",
    "        inter_prodir = inter_dir + scene_name + '_mlwater/'\n",
    "        os.makedirs(inter_prodir, exist_ok=True)\n",
    "        out_mask_prod = inter_prodir + scene_name + '_watermask.tif'\n",
    "        out_prob_prod = inter_prodir + scene_name + '_waterprob.tif'\n",
    "        output_crs = f\"EPSG:{X_t.attrs['crs'].split(':')[-1]}\"\n",
    "        export_xarray_to_geotiff(X_t, out_mask_prod, bands=['water_mask'], crs=output_crs, x_coord='x', y_coord='y', no_data=-9999)\n",
    "        export_xarray_to_geotiff(X_t, out_prob_prod, bands=['water_prob'], crs=output_crs, x_coord='x', y_coord='y', no_data=-9999)\n",
    "        \n",
    "        # CREATE YML\n",
    "        create_yaml(inter_prodir, yaml_prep_water(inter_prodir, yml_meta)) # assumes majority of meta copied from original product yml\n",
    "        \n",
    "        # UPLOAD\n",
    "        s3_upload_cogs(glob.glob(f'{inter_prodir}*'), s3_bucket, s3_dir)\n",
    "        \n",
    "        class_data = None\n",
    "        att = None\n",
    "        img_data = None\n",
    "        clearskymask_img = None\n",
    "        clearskymask_class = None\n",
    "        clearskymask_train = None\n",
    "        class_data = None\n",
    "        train_data = None\n",
    "        wrapperper = None\n",
    "        X = None     \n",
    "        Y = None\n",
    "        pred = None\n",
    "        prob = None\n",
    "        vars_0 = None\n",
    "        X_t = None\n",
    "        \n",
    "        gc.collect()\n",
    "#         shutil.rmtree(inter_prodir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "        class_data = None\n",
    "        att = None\n",
    "        img_data = None\n",
    "        clearskymask_img = None\n",
    "        clearskymask_class = None\n",
    "        clearskymask_train = None\n",
    "        class_data = None\n",
    "        train_data = None\n",
    "        wrapperper = None\n",
    "        X = None     \n",
    "        Y = None\n",
    "        pred = None\n",
    "        prob = None\n",
    "        vars_0 = None\n",
    "        X_t = None\n",
    "\n",
    "        gc.collect()\n",
    "#         shutil.rmtree(inter_prodir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = l8_ymls[5:] + s1_ymls[:5] +s2_ymls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /home/dataprep/data/masks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /home/dataprep/data/*/*/*mask* /home/dataprep/data/masks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optical_yaml_path = l8_ymls[1]\n",
    "# optical_yaml_path = s2_ymls[8]\n",
    "# optical_yaml_path = s1_ymls[8]\n",
    "# summary_yaml_path = wofssummary_ymls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential input vars\n",
    "s3_bucket = 'public-eo-data'\n",
    "s3_dir = 'common_sensing/fiji/mlwater_test/'\n",
    "inter_dir = '../data/'\n",
    "coast_buffer = 'common_sensing/ancillary_products/Buffers/Fiji_1kmBuffer_shp'\n",
    "\n",
    "scene_name = os.path.dirname(optical_yaml_path).split('/')[-1]\n",
    "\n",
    "inter_dir = f\"{inter_dir}{scene_name}_tmp/\"\n",
    "os.makedirs(inter_dir, exist_ok=True)\n",
    "cog_dir = f\"{inter_dir}{scene_name}/\"\n",
    "os.makedirs(cog_dir, exist_ok=True)\n",
    "\n",
    "yml = f'{inter_dir}datacube-metadata.yaml'\n",
    "yml_summary = f'{inter_dir}datacube-metadata_watersummary.yaml'\n",
    "\n",
    "des_band_refs = {\n",
    "    \"LANDSAT_8\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "    \"LANDSAT_7\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "    \"LANDSAT_5\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "    \"LANDSAT_4\": ['blue','green','red','nir','swir1','swir2','pixel_qa'],\n",
    "    \"SENTINEL_2\": ['blue','green','red','nir','swir1','swir2','scene_classification'],\n",
    "    \"SENTINEL_1\": ['vv','vh','layovershadow_mask'],\n",
    "    \"WOFS_SUMMARY\": ['pc']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANDSAT_8 ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'pixel_qa']\n",
      "WOFS_SUMMARY ['pc']\n",
      "swir1 pixel_qa\n"
     ]
    }
   ],
   "source": [
    "# FIND & DOWNLOAD IMAGE DATA\n",
    "if not os.path.exists(yml):\n",
    "    s3_download(s3_bucket, optical_yaml_path, yml)\n",
    "    with open (yml) as stream: yml_meta = yaml.safe_load(stream)\n",
    "    satellite = yml_meta['platform']['code'] # helper to generalise masking \n",
    "    des_bands = des_band_refs[satellite]\n",
    "    print(satellite, des_bands)\n",
    "    band_paths_s3 = [os.path.dirname(optical_yaml_path)+'/'+yml_meta['image']['bands'][b]['path'] for b in des_bands ]\n",
    "    band_paths_local = [inter_dir+os.path.basename(i) for i in band_paths_s3]\n",
    "    for s3, loc in zip(band_paths_s3, band_paths_local): \n",
    "        if not os.path.exists(loc):\n",
    "            s3_download(s3_bucket, s3, loc)\n",
    "elif os.path.exists(yml):\n",
    "    with open (yml) as stream: yml_meta = yaml.safe_load(stream)\n",
    "    satellite = yml_meta['platform']['code'] # helper to generalise masking \n",
    "    des_bands = des_band_refs[satellite]\n",
    "    print(satellite, des_bands)\n",
    "else:\n",
    "    print('boo')\n",
    "\n",
    "# FIND & DOWNLOAD WATER SUMMARY DATA\n",
    "if not os.path.exists(yml_summary):\n",
    "    s3_download(s3_bucket, summary_yaml_path, yml_summary)\n",
    "    with open (yml_summary) as stream: yml_summary_meta = yaml.safe_load(stream)\n",
    "    summary = yml_summary_meta['platform']['code']\n",
    "    des_bands_summary = des_band_refs[summary]\n",
    "    print(summary, des_bands_summary)\n",
    "    band_paths_s3 = [os.path.dirname(summary_yaml_path)+'/'+yml_summary_meta['image']['bands'][b]['path'] for b in des_bands_summary ]\n",
    "    band_paths_local = [inter_dir+os.path.basename(i) for i in band_paths_s3]\n",
    "    for s3, loc in zip(band_paths_s3, band_paths_local): \n",
    "        if not os.path.exists(loc):\n",
    "            s3_download(s3_bucket, s3, loc)\n",
    "elif os.path.exists(yml_summary):\n",
    "    s3_download(s3_bucket, summary_yaml_path, yml_summary)\n",
    "    with open (yml_summary) as stream: yml_summary_meta = yaml.safe_load(stream)\n",
    "    summary = yml_summary_meta['platform']['code']\n",
    "    des_bands_summary = des_band_refs[summary]\n",
    "    print(summary, des_bands_summary)\n",
    "    band_paths_s3 = [os.path.dirname(summary_yaml_path)+'/'+yml_summary_meta['image']['bands'][b]['path'] for b in des_bands_summary ]\n",
    "    band_paths_local = [inter_dir+os.path.basename(i) for i in band_paths_s3]\n",
    "    for s3, loc in zip(band_paths_s3, band_paths_local): \n",
    "        if not os.path.exists(loc):\n",
    "            s3_download(s3_bucket, s3, loc)\n",
    "else:\n",
    "    print('boo')\n",
    "\n",
    "ref_channel = get_ref_channel(satellite)\n",
    "qa_channel = get_qa_channel(satellite)\n",
    "print(ref_channel, qa_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WOFS_SUMMARY', ['pc'], 'swir1')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary, des_bands_summary, ref_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & PREP IMAGE DATA\n",
    "o_bands_data = [ xr.open_rasterio(inter_dir + yml_meta['image']['bands'][b]['path']) for b in des_bands ] # loading\n",
    "o_bands_data = [ resamp_bands(i, o_bands_data) for i in o_bands_data ] # resamp 2 match band 1\n",
    "bands_data = xr.merge([rename_bands(bd, des_bands, i) for i,bd in enumerate(o_bands_data)]).rename({'band': 'time'}) # ensure band names & dims consistent\n",
    "bands_data = bands_data.assign_attrs(o_bands_data[0].attrs) # crs etc. needed later\n",
    "bands_data['time'] = [datetime.strptime(yml_meta['extent']['center_dt'], '%Y-%m-%d %H:%M:%S')] # time dim needed for wofs\n",
    "\n",
    "img_data = bands_data.isel(time = 0).drop(['time'])\n",
    "bands_data = None\n",
    "o_bands_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & PREP WATER SUMMARY DATA\n",
    "o_bands_data = [ xr.open_rasterio(inter_dir + yml_summary_meta['image']['bands'][b]['path']) for b in des_bands_summary ] # loading\n",
    "o_bands_data = [ resamp_bands(i, o_bands_data) for i in o_bands_data ]\n",
    "sum_data = xr.merge([rename_bands(bd, des_bands_summary, i) for i,bd in enumerate(o_bands_data)]).rename({'band': 'time'}) # ensure band names & dims consistent\n",
    "sum_data = sum_data.assign_attrs(o_bands_data[0].attrs) # crs etc. needed later\n",
    "sum_data['time'] = [datetime.strptime(yml_summary_meta['extent']['center_dt'], '%Y-%m-%d %H:%M:%S')] # time dim needed for wofs\n",
    "\n",
    "class_data = sum_data.isel(time = 0).drop(['time']).pc\n",
    "sum_data = None\n",
    "o_bands_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPROJECT & ALIGN CRS+DIMS\n",
    "class_data = class_data.rio.reproject_match(img_data[ref_channel]) # repro + align grid\n",
    "img_data, class_data = xr.align(img_data, class_data, join=\"override\") # force alignment of x,y precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if satellite == 'SENTINEL_1': # catch s1 and scall and re-fromat dtype\n",
    "    att = img_data.attrs\n",
    "    img_data = img_data*100\n",
    "    img_data = img_data.astype('int16')\n",
    "    img_data.attrs = att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID REGION MASKS\n",
    "clearskymask_img = get_valid(img_data, satellite) # img nd mask\n",
    "clearskymask_class = get_valid(class_data, summary) # water nd mask\n",
    "clearskymask_train = clearskymask_class.where(clearskymask_class == False, False) # empty mask\n",
    "clearskymask_train = clearskymask_train.where((clearskymask_img == False) | (clearskymask_class == False), True) # inner true mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGN WATER/NON WATER CLASS LABELS\n",
    "water_thresh = 50 # 50% persistence in annual summary\n",
    "class_data = class_data.where((class_data < water_thresh) | (clearskymask_class == False), 100) # fix > prob to water\n",
    "class_data = class_data.where((class_data >= water_thresh) | (clearskymask_class == False), 0) # fix < prob to no water "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK TO TRAINING SAMPLES W/ IMPUTED ND\n",
    "train_data = img_data # dup as use img 4 implementation later\n",
    "train_data['waterclass'] = class_data # add channel for water mask\n",
    "train_data = train_data.where(clearskymask_train == True, -9999).drop([qa_channel]) # apply inner mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(train_data.waterclass, return_counts=True)\n",
    "\n",
    "if (counts[0] < 500) | (counts[1] < 5000):\n",
    "    raise Exception(f'no class labels should be >5000 for ok classifier. no. training class samples: {counts[0]}{counts[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 2building tree 2 of 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   32.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=5, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=2, n_jobs=2,\n",
       "                       oob_score=False, random_state=None, verbose=2,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPEC & TRAIN MODEL\n",
    "Y = train_data.waterclass.stack(z=['x','y']) # stack into 1-d arr\n",
    "X = train_data.drop(['waterclass']).stack(z=['x','y']).to_array().transpose() # stack into transposed 2-d arr\n",
    "\n",
    "# very shallow classifier - this is a super easy problem & we want it to be fast\n",
    "wrapper = wrap(RandomForestClassifier(n_estimators=4, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt',\n",
    "                               max_depth=5,\n",
    "                               n_jobs=2,\n",
    "                               verbose=2\n",
    "                              ))\n",
    "wrapper.estimator.fit(X, Y) # do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK TO FULL VALID IMAGE FOR IMPLEMENTATION\n",
    "img_data = img_data.drop([qa_channel,'waterclass']) # not sure how these ended up in here(?)\n",
    "img_data = img_data.where(clearskymask_img == True, -9999) # apply just the img mask this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.4s finished\n"
     ]
    }
   ],
   "source": [
    "# PREDICT + ASSIGN CONFIDENCE\n",
    "X = img_data.stack(z=['x','y']).to_array().transpose() # stack into transposed 2-d arr\n",
    "\n",
    "pred = wrapper.estimator.predict(X) # gen class predictions\n",
    "pred[pred==100] = 1\n",
    "prob = wrapper.estimator.predict_proba(X)[:,1]*100 # gen confidence in assigned labels as int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESHAPE OUTPUTS INTO IMAGE\n",
    "vars_0 = [i for i in X.transpose().to_dataset(dim='variable').data_vars] # get list of vars within img\n",
    "X_t = X.transpose().to_dataset(dim='variable') # recreate xrds (but no unstacking yet as need to drop in model outputs)\n",
    "X_t[vars_0[0]].data = pred # add class predictions as first channel\n",
    "X_t[vars_0[2]].data = prob # add confidence as second channel\n",
    "X_t = X_t.rename({vars_0[0]:'water_mask',vars_0[1]:'water_prob'}).drop(vars_0[2:]).unstack('z').transpose().astype('int16') # rename + drop vars + unstack xy dims back to 3-d xrds + transpose predictions back into correct orientation\n",
    "X_t = X_t.where(clearskymask_img,-9999) # ensure probs rm 4 nd regions\n",
    "X_t.attrs = img_data.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_prodir = inter_dir + scene_name + '_mlwater/'\n",
    "os.makedirs(inter_prodir, exist_ok=True)\n",
    "out_mask_prod = inter_prodir + scene_name + '_watermask.tif'\n",
    "out_prob_prod = inter_prodir + scene_name + '_waterprob.tif'\n",
    "output_crs = f\"EPSG:{X_t.attrs['crs'].split(':')[-1]}\"\n",
    "export_xarray_to_geotiff(X_t, out_mask_prod, bands=['water_mask'], crs=output_crs, x_coord='x', y_coord='y', no_data=-9999)\n",
    "export_xarray_to_geotiff(X_t, out_prob_prod, bands=['water_prob'], crs=output_crs, x_coord='x', y_coord='y', no_data=-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yaml(inter_prodir, yaml_prep_water(inter_prodir, yml_meta)) # assumes majority of meta copied from original product yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_upload_cogs(glob.glob(f'{inter_prodir}*'), s3_bucket, s3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "cmd = f'python3 ./utils/dataset_index_from_s3_bucket.py {s3_bucket} -p {s3_dir}{scene_name + \"_mlwater\"}/ --endpoint_url=\"http://s3-uk-1.sa-catapult.co.uk\" --unsigned_requests --start_date 1960-01-01 --end_date 2030-01-01'\n",
    "p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "out = p.stdout.read()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "ax.imshow(img_data.where(clearskymask_img)[ref_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "ax.imshow(X_t.where(X_t.water_mask>=0).water_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "ax.imshow(X_t.where(X_t.water_prob>=0).water_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
